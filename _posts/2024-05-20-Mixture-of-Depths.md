 ---
title: "Accelerating Transformers via Conditional Computation: As Aspect of Mixture of Depths"

author:
 - "Inkwan Hwang"
 - "Minjae Park"

type: docs
bookToc: True
weight: 1
---
*Posted by: Inkwan Hwang, Minjae Park*

# Introduction
“Choice and concentration” is an effective strategies for achieving success in problems. Sometimes, it is not necessary to consume same amount of effort and time into all problems. Expend energy on trivial issues may fail to concentrate on what truly matters. Similarly, in language models, there is a technique that does not focus equally on all tokens but allocates less budget to non-essential tokens. This technique is called conditional computation.

In this post, We will explain a conditional computation strategy for Transformers, focusing on a technology announced this year called **Mixture-of-Texture.**


paper: Mixture-of-Depths: [<U>Dynamically allocating compute in transformer-based language models</U>](https://arxiv.org/abs/2404.02258)


Let's dive in!


## Conditional Computation for transformers
- Early exiting
- CoLT5
  
## Overview to Mixture-of-Depths (MoD)

## Capacity based routing schemes

## Implementation detail

## Open source MoD

## Conclusion and discussion

## Some resources
