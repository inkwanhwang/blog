 ---
title: "Accelerating Transformers via Conditional Computation"
author:
 - "Inkwan Hwang"
 - "Minjae Park"
type: docs
bookToc: True
weight: 1
---
*Posted by: Inkwan Hwang, Minjae Park*

# Introduction
“Choice and concentration” is an effective strategies for achieving success in problems. Sometimes, it is not necessary to consume same amount of effort and time into all problems. Expend energy on trivial issues may fail to concentrate on what truly matters. Similarly, in language models, there is a technique that does not focus equally on all tokens but allocates less budget to non-essential tokens. This technique is called conditional computation.

In this post, We will explain the development of conditional computation for Transformers, focusing on a technology announced this year called [<U>**Mixture-of-Texture.**</U>](https://arxiv.org/abs/2404.02258)

Let's dive in!


## Conditional Computation for transformers
- Early exiting
- CoLT5
  
## Overview to Mixture-of-Depths (MoD)

## Capacity based routing schemes

## Implementation detail

## Open source MoD

## Conclusion and discussion

## Some resources
