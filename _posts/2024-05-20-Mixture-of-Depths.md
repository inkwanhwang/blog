---
title: "Accelerating Transformers via Conditional Computation"
author:
 - "A"
 - "B"
type: docs
bookToc: True
weight: 1
---
*Posted by: Inkwan Hwang, Minjae Park*

# Introduction
“Choice and concentration” is an effective strategies for achieving success in problems. Sometimes, it is not necessary to consume same amount of effort and time into all problems. Expend energy on trivial issues may fail to concentrate on what truly matters. Similarly, in language models, there is a technique that does not focus equally on all tokens but allocates less budget to non-essential tokens. This technique is called <U>[conditional computation](https://arxiv.org/abs/2404.02258)</U>.

In this post, We will explain the development of conditional computation in Transformers, focusing on a technology announced this year called **Mixture-of-Texture**.

Let's dive in!


## Overview to Mixture-of-Depths (MoD)

<p align="center">
    <img src=../images/Mixture-of-Depths_Transformer.png> 
</p>

## Conditional computation methods for transformers

## Capacity based routing schemes

## Implementation detail

## Open source MoD

## Conclusion and discussion

## Some resources
